{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-threshold Neuron - Part 1\n",
    "\n",
    "In this first notebook I will try to formulate the simple maths and pseudocode to implement the concept of a new type of neuron proposed last year in July.\n",
    "\n",
    "I have not check if this idea already exist in a code implementation. I would imagine it does in some form but I prefer to have a nice time without peeking into the answer :).\n",
    "\n",
    "![model proposal comparison](images/model_proposal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "There are some main concepts in the Deep Learning domain that you should be familiar with before procedding. If you are familiar with them skip this part.\n",
    "\n",
    "**Artificial neuron**\n",
    "\n",
    "<img align='left'src=\"images/neuron_comparison.png\" width=\"300px\">\n",
    "A mathematical representation of a biological neuron. They are the corner stone of artificial neural networks and Deep Learning in general. The idea is that the artificial neuron receives input signals from other connected artificial neurons and via a non-linear transmission function emits a signal itself. This transmission function is commonly known as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation function**\n",
    "\n",
    "<img align='left'src=\"images/relu.png\" width=\"300px\">\n",
    "The current understanding of a neuron is that it will transmit some signal only if the sum from incoming signals from other neurons exceeds a threshold. For an artificial neuron this is done via an activation function. There are many of them, and they also add non-linearity to the artificial neural network. There are many [activation functions](https://en.wikipedia.org/wiki/Activation_function) but the Rectified linear unit (ReLu) is recently one of the most broadly used in the Deep Learning community, and it's the one I will use in this notebook. The strict mathematical definition of the function is:\n",
    "\n",
    "$$R(z) = max(0, z) =\n",
    "     \\begin{cases}\n",
    "       0 &\\quad\\text{for } z\\leq0 \\\\\n",
    "       z &\\quad\\text{for } z > 0\n",
    "     \\end{cases}$$\n",
    "     \n",
    "You can check it out in the the official [Tensorflow code](https://github.com/tensorflow/tensorflow/blob/48be6a56d5c49d019ca049f8c48b2df597594343/tensorflow/compiler/tf2xla/kernels/relu_op.cc#L37) or in the [Tensorflow playground](https://github.com/tensorflow/playground/blob/718a6c8f2f876d5450b105e269534ae58e70223d/nn.ts#L120). (as you can notice the derivative is not continuous on $z=0$, this actually caused a bit of hessitation to adopt the activation function due to the fear of \"braking\" back-propagation which is based on derivating all elements of a neural network. So actually the derivative has to be written explicitly check tensorflow source code.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific background\n",
    "\n",
    "S. Sardi *et al.* publish in July last year an experimental work in [Nature scientific reports](https://www.nature.com/srep/) which contradicts a century old assumption about how neurons work. The work was a combined effort between the Physics, Life Sciences and Neuroscience departments of Bar-Ilan University in Tel Aviv.\n",
    "\n",
    "![article_title](images/article.png)\n",
    "\n",
    "The proposed neurons models contain a transmission function which in Deep Learning we refer to as [activation functions](https://en.wikipedia.org/wiki/Activation_function). The authors specifically refer to this function as the neuronal equations.  Their study put to the test three different conceptual neuron models, which I will name: centralized-single-threshold-neuron, centralized-multi-threshold-neuron and decentralized-multi-threshold-neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Centralized-Single-Threshold-Neuron**\n",
    "\n",
    "This is the current adopted computational description of neurons ([artificial neurons](https://en.wikipedia.org/wiki/Artificial_neuron)), and the corner stone of Deep Learning. \"A neuron consists of a unique centralized excitable mechanism\". The signal reaching the neuron consists of a linear sum of the incoming signals from all the dendrites connected to the neuron. If this sum reaches a threshold, a spike signal is propagated through the axiom to the other connected neurons.\n",
    "\n",
    "The neuronal equation of this model is:\n",
    "\n",
    "$$I = \\Theta\\Big(\\sum_{i=1}^NW_i\\cdot I_i - t\\Big)$$\n",
    "\n",
    "where\n",
    "- $i$: identifies any connected neuron\n",
    "- $N$: total number of connected neurons\n",
    "- $W_i$: is the weight (strenght) associated to the connection with neuron $i$\n",
    "- $I_i$: is the signal comming out of neuron $i$\n",
    "- $t$: is the centralized single neuron threshold \n",
    "- $\\Theta$: is the heavyside step function\n",
    "- $I$: signal output from the neuron\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Centralized-multi-threshold-neuron**\n",
    "\n",
    "As for the Centralized-Single-Threshold-Neuron there is a centralized threshold applied by an activation function. The difference is that this model assumess non-linearity signal transmission in each dendrite and a threshold unit for each dendrite. This model is the most complex of the three and its neuronal equation can be written as:\n",
    "\n",
    "$$I=\\Theta(s - t)$$\n",
    "$$s = \\sum_{i=1}^Nf_i(W_i\\cdot I_i)\\cdot\\Theta(W_i\\cdot I_i - t_i)$$\n",
    "\n",
    "where\n",
    "- $i$: identifies any connected neuron\n",
    "- $N$: total number of connected neurons\n",
    "- $W_i$: is the weight (strenght) associated to the connection with neuron $i$\n",
    "- $I_i$: is the signal comming out of neuron $i$\n",
    "- $t_i$: is the threshold value for each neuron $i$\n",
    "- $\\Theta$: is the heavyside step function\n",
    "- $f_i$: non-linear transfer function for connection $i$\n",
    "- $t$: is the centralized single neuron threshold \n",
    "- $I$: signal output from the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decentralized-Multi-Threshold-Neuron**\n",
    "\n",
    "In this model the centralized threshold applied by the centralized activation function is removed. The neuron can be independently excited by any singal coming from a dendrite given that this signal is above a threshold. In escence this model describes a multi-threshold neuron. The mathematical representation is much simpler than the centralized-multi-threshold-neuron and it reads:\n",
    "\n",
    "$$I=\\sum_{i=1}^N\\Theta(W_i\\cdot I_i - t_i)$$\n",
    "(here the authors write an OR, so this equation is correct if they meant the logical OR and not XOR.)\n",
    "\n",
    "where\n",
    "- $i$: identifies any connected neuron\n",
    "- $N$: total number of connected neurons\n",
    "- $W_i$: is the weight (strenght) associated to the connection with neuron $i$\n",
    "- $I_i$: is the signal comming out of neuron $i$\n",
    "- $t_i$: is the threshold value for each neuron $i$\n",
    "- $\\Theta$: is the heavyside step function\n",
    "- $I$: signal output from the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Study conclusion**\n",
    "\n",
    "Based on their experiments the authors conclude that the **Decentralized-Multi-Threshold-Neuron** model explains the data. The authors mention that the main reason for adopting the Centralized-Single-Threshold-Neuron as the main model, is that technology did not allow for direct excitation of single neurons, which other model experiments require. Moreover they state that these results could have been discovered using tehcnology that existed since the 1980s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My idea is to take the Decentralized-Multi-Threshold-Neuron model and try to write an Deep Learning implementation, that is a neural network which will consist of multi-threshold neurons. From the current tools available for Deep Learning Tensorflow is quite flexible and allows for writting user defined implementations, if for some reason the flexibility is not there I will have to write the neural network myself (I hope not, even though I will recommend doing this at least once to really understand how the pieces of a neural network fit togethter.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximations\n",
    "\n",
    "I am a theoretical physicist and as such it's impossible for me not to look for the [spherical cow](https://en.wikipedia.org/wiki/Spherical_cow).\n",
    "\n",
    "**Single threshold value**\n",
    "\n",
    "The multi-threshold neuron model contains different threshold parameter values ($t_i$). Mathematically a threshold has the same effect if we take it as a constant and instead the input signal is moved up or down by the connecting weights parameters. Hence, the neuronal equation becomes:\n",
    "\n",
    "$$I=\\sum_{i=1}^N\\Theta(W_i\\cdot I_i - t)$$\n",
    "\n",
    "(this is also happening in the current neural network implementations, since in reality there is no reason for different neurons to have the same threshold, nevertheless commonly a single activation function is used on all neurons.)\n",
    "\n",
    "**ReLu activation function**\n",
    "\n",
    "The authors mention the neuronal equation, here I will take a step forward and define what in neuroscience is a transmission function. I basically will replace the heavisyde step function ($\\Theta$) with threshold $t$ by a Rectified Linear Unit ($\\mathcal{R}$). I write the transmission function as:\n",
    "\n",
    "$$I=\\sum_{i=1}^N\\mathcal{R}(W_i\\cdot I_i)$$\n",
    "\n",
    "In general any activation function could replace the heavyside step function.\n",
    "\n",
    "**Zero bias**\n",
    "\n",
    "Notice that the proposed model equation contains no bias terms. For simplicity I will keep all bias equal to zero. Hopefully as a second iteration of the code architecture I will be able to add them easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "In order for my neural network to be trained I will probably need backpropagation at some point, this means that the derivative of whatever function I introduce is necessary. Lucky for me I'm not changing the activation function equation itself, so I just use the already derivative of the ReLu function in Tensorflow:\n",
    "\n",
    "$$\\frac{d}{dz}\\mathcal{R}(z)=\n",
    "     \\begin{cases}\n",
    "       0 &\\quad\\text{for } z\\leq0 \\\\\n",
    "       1 &\\quad\\text{for } z > 0\n",
    "     \\end{cases}$$\n",
    "     \n",
    "You can check it out in the the official [Tensorflow code](https://github.com/tensorflow/tensorflow/blob/48be6a56d5c49d019ca049f8c48b2df597594343/tensorflow/compiler/tf2xla/kernels/relu_op.cc#L63) or in the [Tensorflow playground](https://github.com/tensorflow/playground/blob/718a6c8f2f876d5450b105e269534ae58e70223d/nn.ts#L121)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor multiplication\n",
    "\n",
    "What I'm really changing is the architecture of the neural network as seen in Figure 0,  the activation function is no longer applied on the sum of all the inputs from the connected neurons, but instead on each input arriving from a connected neuron. In a simple description the sum over the arriving signal is going from inside the activation function to outside of it:\n",
    "\n",
    "\n",
    "$$\\mathcal{R}\\Big(\\sum_{i=1}^NW_i\\cdot I_i\\Big) \\rightarrow \\sum_{i=1}^N\\mathcal{R}(W_i\\cdot I_i)$$\n",
    "\n",
    "Do you see the implementation problem described by the equation above?\n",
    "\n",
    "In the current model (left equation) the input to the activation function $\\sum_iW_i\\cdot I_i$ is exactly the dot product between vectors $(W_1, W_2,\\dots,W_N)$ and $(I_1, I_2,\\dots,I_N)$ and it's this fact which allows fast computation of input signals for many neurons via matrix multiplication. In the fully connected layer for example, the input signal for $j$ neurons comming from connected neurons $i$ is $\\sum_iW_{ji}\\cdot I_i$, which means we can calculate the input signal for all $j$ neurons with one simple matrix multiplication $W \\cdot I$ (given that $I$ is written as a $N\\times1$ matrix).\n",
    "\n",
    "In the new model this is no longer possible. I think this will be the biggest challenge, but AFAIK Tensorflow should provide enough flexibility to write this architecture. The idea is to keep things in the tensorflow world as all the functions in the package can be added to calculations and gradients will be automatically available for backpropagation.\n",
    "\n",
    "It is also interesting to think about if the fact that we can do fast matrix multiplications lead us to prefer the current central-threshold model, like the modified Maslow's hammer saying goes \"if all you have is a hammer, everything looks like a nail\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tensorflow `relu_layer` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.constant(2, shape=(10,10))\n",
    "x = tf.constant(2, shape=(1, 10))\n",
    "b = tf.constant(0, shape=(10,))\n",
    "I = tf.nn.relu_layer(x, w , b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 40, 40, 40, 40, 40, 40, 40, 40, 40]], dtype=int32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow relu function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(10, shape=(10,))\n",
    "I = tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
