{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-threshold Neuron - Part 1\n",
    "\n",
    "![model proposal comparison](images/model_proposal.png)\n",
    "<sup>Figure 0. Schematic diagrams of two neuron models. (Central-threshold neuron) The model on the left is the current employed model in artifical naural networks where the input signals are propagated if there sum is above certain treshold. (Multi-threshold neuron) In contrast, in the right I show the new proposed model where each input signal goes through a threshold filter before summing them.</sup>\n",
    "\n",
    "In this first notebook I construct and train a simple Deep Neural Network based on a novel experimental driven neuron model proposed last year in July[0]. The notebook is separated as follows:\n",
    "\n",
    "- Neuroscience background\n",
    "    - Summarize the article that lead me to this idea and explain some of the theory.\n",
    "- Concepts\n",
    "    - Relate Deep Learning technical concepts to Neuroscience concepts mentioned. \n",
    "- Approximations\n",
    "    - Introduce approximations I will make on the multi-threshold neuron model.\n",
    "- Discussion\n",
    "    - Overview of mathematical and Deep Learning implications as a consequence of the multi-threshold neuron model.\n",
    "- Code\n",
    "    - Tensorflow implementation and training of a simple fully-connected Deep Neural Network using the multi-threshold neuron model.\n",
    "\n",
    "\n",
    "[0]: I have not check if this idea already exists in a code implementation. I would imagine it does in some form but I prefer to have a nice time without peeking into the answer :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuroscience background\n",
    "\n",
    "![article_title](images/article.png)\n",
    "\n",
    "S. Sardi *et al.* published in July last year an experimental work in [Nature scientific reports](https://www.nature.com/articles/s41598-017-18363-1) which contradicts a century old assumption about how neurons work. The work was a combined effort between the Physics, Life Sciences and Neuroscience departments of Bar-Ilan University in Tel Aviv, Israel.\n",
    "\n",
    "\n",
    "The authors proposed three different neuron models which they put to the test with different types of experiments. They describe each neuron model with, what they call, *neuronal equations*.\n",
    "\n",
    "![neuron](images/neuron.jpg)\n",
    "<sup>Figure 1. Schematic representation of a neuron. The signal in a neural network flows from a neuron's axon to the dendrites of another one. That is, the signal in any neuron is incomming form its dendrites and outgoing to its axon.</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Central-threshold neuron**\n",
    "\n",
    "This is the current adopted computational description of neurons ([artificial neurons](https://en.wikipedia.org/wiki/Artificial_neuron)), and the corner stone of Deep Learning. \"A neuron consists of a unique centralized excitable mechanism\". The signal reaching the neuron consists of a linear sum of the incoming signals from all the dendrites connected to the neuron, if this sum reaches a threshold, a spike signal is propagated through the axom to the other connected neurons.\n",
    "\n",
    "The neuronal equation of this model is:\n",
    "\n",
    "$$I = \\Theta\\Big(\\sum_{i=1}^NW_i\\cdot I_i - t\\Big)$$\n",
    "\n",
    "where\n",
    "- $i$: identifies any connected neuron\n",
    "- $N$: total number of connected neurons\n",
    "- $W_i$: is the weight (strenght) associated to the connection with neuron $i$\n",
    "- $I_i$: is the signal comming out of neuron $i$\n",
    "- $t$: is the centralized single neuron threshold \n",
    "- $\\Theta$: is the [heavyside step function](https://en.wikipedia.org/wiki/Heaviside_step_function)\n",
    "- $I$: signal output from the neuron\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Central-multi-threshold neuron**\n",
    "\n",
    "As for the central-threshold neuron there is a centralized threshold applied by an activation function. The difference is that this model assumess non-linearity signal transmission and a threshold unit in each dendrite. This model is the most complex of the three and its neuronal equation can be written as:\n",
    "\n",
    "$$I=\\Theta(s - t)$$\n",
    "$$s = \\sum_{i=1}^Nf_i(W_i\\cdot I_i)\\cdot\\Theta(W_i\\cdot I_i - t_i)$$\n",
    "\n",
    "where\n",
    "- $i$: identifies any connected neuron\n",
    "- $N$: total number of connected neurons\n",
    "- $W_i$: is the weight (strenght) associated to the connection with neuron $i$\n",
    "- $I_i$: is the signal comming out of neuron $i$\n",
    "- $t_i$: is the threshold value for each neuron $i$\n",
    "- $\\Theta$: is the heavyside step function\n",
    "- $f_i$: non-linear transfer function for connection $i$\n",
    "- $t$: is the centralized single neuron threshold \n",
    "- $I$: signal output from the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-threshold neuron**\n",
    "\n",
    "In this model the centralized threshold is removed. The neuron can be independently excited by any singal coming from a dendrite given that this signal is above a threshold. This model describes a multi-threshold neuron and the mathematical representation can be written as:\n",
    "\n",
    "$$I=\\sum_{i=1}^N\\Theta(W_i\\cdot I_i - t_i)$$\n",
    "(here the authors write an OR, so this equation is correct if they meant the logical OR and not XOR.)\n",
    "\n",
    "where\n",
    "- $i$: identifies any connected neuron\n",
    "- $N$: total number of connected neurons\n",
    "- $W_i$: is the weight (strenght) associated to the connection with neuron $i$\n",
    "- $I_i$: is the signal comming out of neuron $i$\n",
    "- $t_i$: is the threshold value for each neuron $i$\n",
    "- $\\Theta$: is the heavyside step function\n",
    "- $I$: signal output from the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Study conclusion**\n",
    "\n",
    "Based on their experiments the authors conclude that the **Multi-threshold neuron** model explains the data. The authors mention that the main reason for adopting the central-threshold neuron as the main model, is that technology did not allow for direct excitation of single neurons, which other model experiments require. Moreover, they state that these results could have been discovered using technology that existed since the 1980s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "There are some main concepts in the Deep Learning domain that you should be familiar with before procedding. If you are familiar with them skip this part.\n",
    "\n",
    "**Artificial neuron**\n",
    "\n",
    "<img align='left'src=\"images/neuron_comparison.png\" width=\"300px\">\n",
    "A mathematical representation of a biological neuron. They are the corner stone of artificial neural networks and Deep Learning. The idea is that the artificial neuron receives input signals from other connected artificial neurons and via a non-linear transmission function emits a signal itself. This transmission function is commonly known as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation function**\n",
    "\n",
    "<img align='left'src=\"images/relu.png\" width=\"300px\">\n",
    "The current understanding of a neuron is that it will transmit some signal only if the sum from incoming signals from other neurons exceeds a threshold. For an artificial neuron this threshold filter is applied via an activation function. There are many [activation functions](https://en.wikipedia.org/wiki/Activation_function) but the Rectified linear unit (ReLu) is recently one of the most broadly used in the Deep Learning community, and it's the one I will use in this notebook. The strict mathematical definition of the function is:\n",
    "\n",
    "$$R(z) = max(0, z) =\n",
    "     \\begin{cases}\n",
    "       0 &\\quad\\text{for } z\\leq0 \\\\\n",
    "       z &\\quad\\text{for } z > 0\n",
    "     \\end{cases}$$\n",
    "     \n",
    "You can check its implementation in the the official [Tensorflow source code](https://github.com/tensorflow/tensorflow/blob/48be6a56d5c49d019ca049f8c48b2df597594343/tensorflow/compiler/tf2xla/kernels/relu_op.cc#L37) or in the [Tensorflow playground code](https://github.com/tensorflow/playground/blob/718a6c8f2f876d5450b105e269534ae58e70223d/nn.ts#L120).[1]\n",
    "\n",
    "\n",
    "[1]: (as you can notice the derivative is not continuous on $z=0$. Among other things, this actually caused a bit of hesitation since the most commonly used algorithm for training an artificial neural network is backpropagation, which consists on calculating gradients of all elements in the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My idea is to take the Decentralized-Multi-Threshold-Neuron model and try to write an Deep Learning implementation, that is a neural network which will consist of multi-threshold neurons. From the current tools available for Deep Learning Tensorflow is quite flexible and allows for writting user defined implementations, if for some reason the flexibility is not there I will have to write the neural network myself (I hope not, even though I will recommend doing this at least once to really understand how the pieces of a neural network fit togethter.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximations\n",
    "\n",
    "![cow](images/spherical_cow.gif)\n",
    "\n",
    "I am a theoretical physicist and as such it's impossible for me to resist the [spherical cow](https://en.wikipedia.org/wiki/Spherical_cow).\n",
    "\n",
    "**Single threshold value**\n",
    "\n",
    "The multi-threshold neuron model contains different threshold parameter values ($t_i$). Mathematically a threshold has the same effect if I take it as a constant and instead the input signal is moved up or down by the connecting weight parameters. Hence, the neuronal equation becomes:\n",
    "\n",
    "$$I=\\sum_{i=1}^N\\Theta(W_i\\cdot I_i - t)$$\n",
    "\n",
    "(this is also happening in the current neural network implementations, since in reality there is no reason for different neurons to have the same threshold, nevertheless commonly a single activation function is used on all neurons.)\n",
    "\n",
    "**ReLu activation function**\n",
    "\n",
    "I'll define what in neuroscience is a transmission function. I basically will replace the heavisyde step function ($\\Theta$) with threshold $t$ by a [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) ($\\mathcal{R}$). I write the transmission function as:\n",
    "\n",
    "$$I=\\sum_{i=1}^N\\mathcal{R}(W_i\\cdot I_i)$$\n",
    "\n",
    "In general any activation function could replace the heavyside step function.\n",
    "\n",
    "**Bias**\n",
    "\n",
    "Notice that the proposed model equation contains no bias terms. I'll add a bias term to the equation since it's known to help neural networks fit better.\n",
    "\n",
    "$$I=\\sum_{i=1}^N\\mathcal{R}(W_i\\cdot I_i) + b$$\n",
    "\n",
    "It would be nice to see if with this architecture the bias term is as important as for the central-threshold neuron model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "In order for my neural network to be trained I will probably need backpropagation at some point, this means that the derivative of whatever function I introduce is necessary. Lucky for me I'm not changing the activation function itself, I can just use the already derivative of the ReLu function in Tensorflow:\n",
    "\n",
    "$$\\frac{d}{dz}\\mathcal{R}(z)=\n",
    "     \\begin{cases}\n",
    "       0 &\\quad\\text{for } z\\leq0 \\\\\n",
    "       1 &\\quad\\text{for } z > 0\n",
    "     \\end{cases}$$\n",
    "     \n",
    "You can check it out in the the official [Tensorflow source code](https://github.com/tensorflow/tensorflow/blob/48be6a56d5c49d019ca049f8c48b2df597594343/tensorflow/compiler/tf2xla/kernels/relu_op.cc#L63) or in the [Tensorflow playground code](https://github.com/tensorflow/playground/blob/718a6c8f2f876d5450b105e269534ae58e70223d/nn.ts#L121)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor multiplication**\n",
    "\n",
    "What I'm really changing is the architecture of the artificial neural network as seen in Figure 0,  the activation function is no longer applied on the sum of all the inputs from the connected neurons, but instead on each input arriving from a connected neuron. The sum operation is going from inside the activation function to outside of it:\n",
    "\n",
    "\n",
    "$$\\mathcal{R}\\Big(\\sum_{i=1}^NW_i\\cdot I_i\\Big) \\rightarrow \\sum_{i=1}^N\\mathcal{R}(W_i\\cdot I_i)$$\n",
    "\n",
    "Do you see the implementation problem described by the equation above?\n",
    "\n",
    "In the central-threshold model (left equation) the input to the activation function $\\sum_iW_i\\cdot I_i$ is exactly the dot product between vectors $(W_1, W_2,\\dots,W_N)$ and $(I_1, I_2,\\dots,I_N)$ and it's this fact which allows fast computation of input signals for many neurons via matrix multiplication. In a fully connected layer architecture for example, the input signal to neuron $j$ comming from connected neurons $i$ is $\\sum_i I_i\\cdot W_{ij}$, which means I can calculate the input signal for all $j$ neurons with a single matrix multiplication $I \\cdot W$.\n",
    "\n",
    "In the multi-threshold model this is no longer possible. I think this will be the biggest challenge, but AFAIK Tensorflow should provide enough flexibility to write this architecture. The idea is to keep things in the tensorflow world as all the functions in the package can be added to calculations and gradients will be automatically available for backpropagation. [3]\n",
    "\n",
    "[3]: It is also interesting to think about if the fact that we can do fast matrix multiplications lead us to computationaly prefer the current central-threshold model, like the modified Maslow's hammer saying goes \"if all you have is a hammer, everything looks like a nail\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I have the following weight matrix connecting two neuron layers, the first layer has 3 neurons the second with 2:\n",
    "\n",
    "$$W=\n",
    "\\begin{bmatrix}\n",
    "    3 & -4 \\\\\n",
    "    -2& 2\\\\\n",
    "    0& 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and that the output signal from the neurons in the first layer are\n",
    "\n",
    "$$I_0=\n",
    "\\begin{bmatrix}\n",
    "    2 & 5 & 1  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with bias terms\n",
    "\n",
    "$$b=\n",
    "\\begin{bmatrix}\n",
    "    2 & -1  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using the standard central-threshold neuron model the output signal of the second layer is:\n",
    "\n",
    "$$\\mathcal{R}\\Big(I_0\\cdot W + b\\Big) = \\mathcal{R}\\Big(\n",
    "\\begin{bmatrix}\n",
    "    2 & 5 & 1  \n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "    3 & -4 \\\\\n",
    "    -2& 2\\\\\n",
    "    0& 4\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    2 & -1  \n",
    "\\end{bmatrix}\n",
    "\\Big)\n",
    "=\n",
    "\\mathcal{R}\\Big(\n",
    "\\begin{bmatrix}\n",
    "    -2 & 5 \n",
    "\\end{bmatrix}\n",
    "\\Big)\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    \\mathcal{R}(-2)& \\mathcal{R}(5)\n",
    "\\end{bmatrix}\n",
    "\\Big)\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    0 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the case of the multi-threshold neuron model proposed the output is\n",
    "\n",
    "$$\\mathcal{R}\\Big(\n",
    "\\begin{bmatrix}\n",
    "    2 & 5 & 1  \n",
    "\\end{bmatrix}\n",
    "\\bigcirc\n",
    "\\begin{bmatrix}\n",
    "    3 & -4 \\\\\n",
    "    -2& 2\\\\\n",
    "    0& 4\n",
    "\\end{bmatrix}\n",
    "\\Big)\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    2 & -1  \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    \\mathcal{R}(6) + \\mathcal{R}(-10) + \\mathcal{R}(0) + 2 & \\mathcal{R}(-8) + \\mathcal{R}(10) + \\mathcal{R}(4)  -1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    8 & 13 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here I have abused the symbols but the operation $\\bigcirc$ in combination with $\\mathcal{R}$ represent the non-linear transmission function of the multi-threshold neuron model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the example shows, a fundamental difference is that in the multi-threshold case if any term (neuron output signal times weight) is positive then the output will be positive. This will greatly reduce the sparsity of the neurons firing throughout the network in comparison with the conventional central-threshold model.\n",
    "\n",
    "I don't know all the implications but I expect that it will be more difficult for individual neurons (or parts of the network) to singly address a specific feature, therefore reducing overfitting. Something similar is achieved in current Deep Neural Networks using the [Drop Out technique](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout).\n",
    "\n",
    "A known issue of most activation functions in Deep Neural Networks is the \"vanishing gradient problem\", it relates to the decreasing update value to the weights as the errors propagate through the network via backpropagation. In the standard central-threshold model the ReLu partially solves this problem by having a derivative equal to 1 if the neuron fires, this propagates the error without vanishing the gradient. On the other hand, if the neuron signal is negative and squashed by the ReLu (did not fire) the corresponding weights are not updated, since the ReLu derivate is zero i.e. neuron connections are not learning when they didn't fire but perhaps they shold have. In the multi-threshold model I expect this last issue to be reduced since sparsity reduces, more weights should be updated on each step in comparison with the central-threshold neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Tensorflow contains the built-in realted `ReLu` functions:\n",
    "\n",
    "- `relu_layer`\n",
    "- `relu`\n",
    "\n",
    "The `relu_layer` function already assumes an architecture with central-threshold neurons. The `relu` function on the other hand can operate on each entry of a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replicating the example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigo/anaconda3/envs/multi-neuron/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Central-threshold Neuron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 5]], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.constant([[2, -1]])\n",
    "w = tf.constant([[3, -4], [-2, 2], [0, 4]])\n",
    "I_0 = tf.constant([[2, 5, 1]])\n",
    "I_1 = tf.nn.relu(tf.matmul(I_0, w) + b)\n",
    "I_1.eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-threshold Neuron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 13], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.constant([2, -1])\n",
    "w = tf.constant([[3, -2, 0], [-4, 2, 4]])\n",
    "I_0 = tf.constant([2, 5, 1])\n",
    "I_1 = tf.reduce_sum(tf.nn.relu(tf.multiply(I_0, w)), axis=1) + b\n",
    "I_1.eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replicating example**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "b = tf.constant([2, -1])\n",
    "w = tf.constant([[3, -2, 0], [-4, 2, 4]])\n",
    "I_0 = tf.constant([2, 5, 1])\n",
    "I_1 = tf.reduce_sum(tf.nn.relu(tf.multiply(I_0, w)), axis=1) + b\n",
    "I_1.eval(session=sess)\n",
    ">>> array([ 8, 13], dtype=int32)\n",
    "```\n",
    "\n",
    "Notice that `b` and `I_0` are one dimensional tensors, this allows me to take advantage of the tensorflow broadcasting feature and simplify the code necessary to perform the non-linear operation for the multi-threshold neuron model. Using the example I can then define a neural network layer consisting of multi-threshold neurons.\n",
    "\n",
    "```python\n",
    "def multi_threshold_neuron_layer(input_values, weights, b, activation=tf.nn.relu):\n",
    "    return tf.reduce_sum(activation(tf.multiply(input_values, weights)) + b, axis=1)\n",
    "```\n",
    "\n",
    "[2]: Is a one line function, i know i know, but I can already sense there will be more to it later since this just works for a single input example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST - 2 hidden layer multi-threshold neural network\n",
    "\n",
    "With this basic implementation my goal was just to see the model be able to train, I just wanted to see the loss decrease with each iteration. As you probably noticed, the `multi_threshold_neuron_layer` can only take 1 example at a time, this is the complication when I said that simple matrix multiplication was no longer possible. I expect to extend this code in notebook part II to be able to receive batches thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_threshold_neuron_layer(input_values, weights, b, activation=tf.nn.relu):\n",
    "    return tf.reduce_sum(activation(tf.multiply(input_values, weights)), axis=1) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "n_input = 784 # 28 x 28\n",
    "hlayers_sizes = [251, 87]\n",
    "n_classes = 10 # 0-9 digits\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Construct model\n",
    "I_0 = tf.placeholder(\"float\", shape=(n_input,)) # input layer\n",
    "\n",
    "W_01 = tf.Variable(tf.random_normal((hlayers_sizes[0], n_input)))\n",
    "b_1 = tf.Variable(tf.random_normal((hlayers_sizes[0],)))\n",
    "I_1 = multi_threshold_neuron_layer(I_0, W_01, b_1) # 1st hidden layer\n",
    "\n",
    "W_12 = tf.Variable(tf.random_normal((hlayers_sizes[1], hlayers_sizes[0])))\n",
    "b_2 = tf.Variable(tf.random_normal((hlayers_sizes[1],)))\n",
    "I_2 = multi_threshold_neuron_layer(I_1, W_12, b_2) # 2nd hidden layer\n",
    "\n",
    "W_23 = tf.Variable(tf.random_normal((n_classes, hlayers_sizes[1])))\n",
    "b_3 = tf.Variable(tf.random_normal((n_classes,)))\n",
    "output = multi_threshold_neuron_layer(I_2, W_23, b_3) # output layer\n",
    "\n",
    "# truth\n",
    "target = tf.placeholder(\"float\", shape=(n_classes,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, optimizer and assessing prediction\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=target))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(output), tf.argmax(target))\n",
    "\n",
    "# Initializing variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Examples in Epoch: 27500\n",
      "# Epochs: 10\n"
     ]
    }
   ],
   "source": [
    "n_observations = mnist.train.num_examples // 2\n",
    "\n",
    "print('# Examples in Epoch:', n_observations)\n",
    "print('# Epochs:', n_epochs)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = 0\n",
    "        acc = 0\n",
    "        # Loop over all examples\n",
    "        for i in range(n_observations):\n",
    "            x, y = mnist.train.next_batch(1)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l, c = sess.run([optimizer, loss, correct_prediction], feed_dict={I_0: x[0], target: y[0]})\n",
    "            # Compute average loss\n",
    "            avg_loss += l\n",
    "            acc += c\n",
    "        # Display logs per epoch step\n",
    "        avg_loss /= n_observations\n",
    "        acc /= n_observations\n",
    "        print(f'Epoch: {epoch + 1}\\t Avg Loss: {avg_loss:.3f}\\t Accuracy: {acc:.3f}')\n",
    "        losses.append(avg_loss), accuracies.append(acc)\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Test metrics\n",
    "    accuracy = 0\n",
    "    for i in range(n_observations):\n",
    "        x_test, y_test = mnist.test.next_batch(1)\n",
    "        accuracy += correct_prediction.eval({I_0: x_test[0], target: y_test[0]})\n",
    "    accuracy /= n_observations\n",
    "    print(f\"Accuracy on test set: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
